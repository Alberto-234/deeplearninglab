{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar100\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, LearningRateScheduler, TensorBoard, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import print_summary, to_categorical\n",
    "from keras import backend as K\n",
    "import sys\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "NUM_CLASSES = 100\n",
    "EPOCHS = 165000\n",
    "INIT_DROPOUT_RATE = 0.5\n",
    "MOMENTUM_RATE = 0.9\n",
    "INIT_LEARNING_RATE = 0.01\n",
    "L2_DECAY_RATE = 0.0005\n",
    "CROP_SIZE = 32\n",
    "LOG_DIR = './logs'\n",
    "MODEL_PATH = './models/keras_cifar100_model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to Keras, we can load the dataset easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to convert the labels in the dataset into categorical matrix structure from 1-dim numpy array structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "y_test = to_categorical(y_test, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once bitten twice shy, we will not forget it for this time. We need to normalize the images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following experiments should highlight the generalization capabilities of ELU networks.  The\n",
    "CNN architecture is more sophisticated than in the previous subsection and consists of 18 convolutional layers arranged in stacks of ([1×384×3],[1×384×1,1×384×2,2×640×2],[1×640×1,3×768×2],[1×768×1,2×896×2],[1×896×3,2×1024×2],[1×1024×1,1×1152×2],[1×1152×1],[1×100×1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(ZeroPadding2D(4, input_shape=x_train.shape[1:]))\n",
    "model.add(Conv2D(384, (3, 3), padding='same', kernel_regularizer=l2(0.01)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(INIT_DROPOUT_RATE))\n",
    "\n",
    "model.add(Conv2D(384, (1, 1), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
    "model.add(Conv2D(384, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
    "model.add(Conv2D(640, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
    "model.add(Conv2D(640, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(INIT_DROPOUT_RATE))\n",
    "\n",
    "model.add(Conv2D(640, (3, 3), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
    "model.add(Conv2D(768, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
    "model.add(Conv2D(768, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
    "model.add(Conv2D(768, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(INIT_DROPOUT_RATE))\n",
    "\n",
    "model.add(Conv2D(768, (1, 1), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
    "model.add(Conv2D(896, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
    "model.add(Conv2D(896, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(INIT_DROPOUT_RATE))\n",
    "\n",
    "model.add(Conv2D(896, (3, 3), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
    "model.add(Conv2D(1024, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
    "model.add(Conv2D(1024, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(INIT_DROPOUT_RATE))\n",
    "\n",
    "model.add(Conv2D(1024, (1, 1), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
    "model.add(Conv2D(1152, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(INIT_DROPOUT_RATE))\n",
    "\n",
    "model.add(Conv2D(1152, (1, 1), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(INIT_DROPOUT_RATE))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(NUM_CLASSES))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d_1 (ZeroPaddin (None, 40, 40, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 40, 40, 384)       10752     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 40, 40, 384)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 20, 20, 384)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 20, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 384)       147840    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 20, 384)       590208    \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 20, 20, 640)       983680    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 20, 20, 640)       1639040   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 20, 20, 640)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 10, 10, 640)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10, 10, 640)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 10, 10, 640)       3687040   \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 10, 10, 768)       1966848   \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 10, 10, 768)       2360064   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 10, 10, 768)       2360064   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10, 10, 768)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 768)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 5, 5, 768)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 5, 5, 768)         590592    \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 5, 5, 896)         2753408   \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 5, 5, 896)         3212160   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5, 5, 896)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 896)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 3, 3, 896)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 3, 3, 896)         7226240   \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 3, 3, 1024)        3671040   \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 3, 3, 1024)        4195328   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3, 3, 1024)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2, 2, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 2, 2, 1024)        1049600   \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 2, 2, 1152)        4719744   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2, 2, 1152)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 1, 1, 1152)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1, 1, 1152)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 1, 1, 1152)        1328256   \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 1, 1, 1152)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 1, 1, 1152)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1, 1, 1152)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               115300    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 100)               0         \n",
      "=================================================================\n",
      "Total params: 42,607,204\n",
      "Trainable params: 42,607,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use callbacks to make these adjustments. First, we will write the schedulers for learning rate and the drop-out rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr, step_decay = 0.1):\n",
    "    if epoch == 35000:\n",
    "        return float(lr * step_decay)\n",
    "    elif epoch == -1:\n",
    "        return float(lr * step_decay * 5)\n",
    "    else:\n",
    "        return float(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dr_scheduler(epoch, layers, rate_list = [0.0, .1, .2, .3, .4, .5, 0.0], rate_factor = 1.5):\n",
    "    if epoch == 85000:\n",
    "        for i, layer in enumerate([l for l in layers if \"dropout\" in np.str.lower(l.name)]):\n",
    "            layer.rate = layer.rate + rate_list[i]\n",
    "    elif epoch == 135000:\n",
    "        for i, layer in enumerate([l for l in layers if \"dropout\" in np.str.lower(l.name)]):\n",
    "            layer.rate = layer.rate + layer.rate * rate_factor if layer.rate <= 0.66 else 1\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can define our custom callback objects for the learning rate and drop-out rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepLearningRateSchedulerAt(LearningRateScheduler):\n",
    "    def __init__(self, schedule, verbose = 0): \n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.schedule = schedule\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None): \n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "            \n",
    "        lr = float(K.get_value(self.model.optimizer.lr))\n",
    "        lr = self.schedule(epoch, lr)\n",
    "       \n",
    "        if not isinstance(lr, (float, np.float32, np.float64)):\n",
    "            raise ValueError('The output of the \"schedule\" function ' 'should be float.')\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "        if self.verbose > 0: \n",
    "            print('\\nEpoch %05d: LearningRateScheduler reducing learning ' 'rate to %s.' % (epoch + 1, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutRateScheduler(Callback):\n",
    "    def __init__(self, schedule, verbose = 0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.schedule = schedule\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model, 'layers'):\n",
    "            raise ValueError('Model must have a \"layers\" attribute.')\n",
    "            \n",
    "        layers = self.model.layers\n",
    "        layers = self.schedule(epoch, layers)\n",
    "        \n",
    "        if not isinstance(layers, list):\n",
    "            raise ValueError('The output of the \"schedule\" function should be list.')\n",
    "        \n",
    "        self.model.layers = layers\n",
    "        \n",
    "        if self.verbose > 0:\n",
    "            for layer in [l for l in self.model.layers if \"dropout\" in np.str.lower(l.name)]:\n",
    "                print('\\nEpoch %05d: Dropout rate for layer %s: %s.' % (epoch + 1, layer.name, layer.rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying zero padding four 0 pixels at all borders, we will randomly crop the images by 32x32. To achieve this, we need to create custom generator which takes ImageDataGenerator object as an input and yields each batch of images by cropping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(img, random_crop_size):\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    dy, dx = random_crop_size\n",
    "    x = np.random.randint(0, width - dx + 1)\n",
    "    y = np.random.randint(0, height - dy + 1)\n",
    "    return img[y:(y+dy), x:(x+dx), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_generator(batches, crop_length, num_channel = 3):\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, num_channel))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
    "        yield (batch_crops, batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the optimizer (Mini-batch Stochastic Gradient Descent with Momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(lr=INIT_LEARNING_RATE, momentum=MOMENTUM_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the part that I be loved. Callbacks! Let's create callback objects. First one is our custom learning scheduler to decrease the learning rate after a certain number of epoch. Also, we have another custom callback for adjusting the drop-out rates in the stack layers. Next, we will record what our model has done during the training process. And lastly, we will save our trained model in each epoch that has better result than previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rate_scheduler = StepLearningRateSchedulerAt(lr_scheduler)\n",
    "dropout_scheduler = DropoutRateScheduler(dr_scheduler)\n",
    "tensorboard = TensorBoard(log_dir=LOG_DIR, batch_size=BATCH_SIZE)\n",
    "checkpointer = ModelCheckpoint(MODEL_PATH, monitor='val_loss', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to compile. GO GO GO!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'top_k_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use ImageDataGenerator object to handle the data pre-processing on real time and make sure that the process goes randomly. Just for reminding, in the article, global contrast normalization (sample-wise centering) and ZCA whitening and horizontal flipping methods should be used for augmenting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birdortyedi/Programs/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/preprocessing/image.py:645: UserWarning: This ImageDataGenerator specifies `zca_whitening`, which overrides setting of `featurewise_center`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(samplewise_center=True,\n",
    "                             zca_whitening=True,\n",
    "                             horizontal_flip=True,\n",
    "                             validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTENTION!\n",
    "If we use sample-wise or feature-wise centering methods, we have to fit the training data to the generator. Otherwise, these methods do not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will flow the data using our custom generator object for cropping the images. Here is the flowing methods for training and validation data. Since we define the rate of splitting the data to training and validation in the ImageDataGenerator object, it is enough to specify the subset as \"training\" or \"validation\" in the flowing method to split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flow = datagen.flow(x_train, y_train, batch_size=BATCH_SIZE, subset=\"training\")\n",
    "train_flow_w_crops = crop_generator(train_flow, CROP_SIZE)\n",
    "valid_flow = datagen.flow(x_train, y_train, batch_size=BATCH_SIZE, subset=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOW! Ready to train, huh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/165000\n",
      "500/500 [==============================] - 189s 378ms/step - loss: 10.9127 - acc: 0.0190 - top_k_categorical_accuracy: 0.0891 - val_loss: 10.5332 - val_acc: 0.0251 - val_top_k_categorical_accuracy: 0.1121\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 10.53320, saving model to ./models/keras_cifar100_model.h5\n",
      "Epoch 2/165000\n",
      "500/500 [==============================] - 179s 357ms/step - loss: 9.9548 - acc: 0.0546 - top_k_categorical_accuracy: 0.2079 - val_loss: 9.3235 - val_acc: 0.1090 - val_top_k_categorical_accuracy: 0.3195\n",
      "\n",
      "Epoch 00002: val_loss improved from 10.53320 to 9.32350, saving model to ./models/keras_cifar100_model.h5\n",
      "Epoch 3/165000\n",
      "500/500 [==============================] - 180s 360ms/step - loss: 9.0643 - acc: 0.1030 - top_k_categorical_accuracy: 0.3261 - val_loss: 8.6395 - val_acc: 0.1246 - val_top_k_categorical_accuracy: 0.3754\n",
      "\n",
      "Epoch 00003: val_loss improved from 9.32350 to 8.63947, saving model to ./models/keras_cifar100_model.h5\n",
      "Epoch 4/165000\n",
      "500/500 [==============================] - 179s 359ms/step - loss: 8.3002 - acc: 0.1524 - top_k_categorical_accuracy: 0.4162 - val_loss: 7.7626 - val_acc: 0.2121 - val_top_k_categorical_accuracy: 0.5022\n",
      "\n",
      "Epoch 00004: val_loss improved from 8.63947 to 7.76259, saving model to ./models/keras_cifar100_model.h5\n",
      "Epoch 5/165000\n",
      "500/500 [==============================] - 173s 346ms/step - loss: 7.6244 - acc: 0.1990 - top_k_categorical_accuracy: 0.4947 - val_loss: 7.1418 - val_acc: 0.2583 - val_top_k_categorical_accuracy: 0.5612\n",
      "\n",
      "Epoch 00005: val_loss improved from 7.76259 to 7.14185, saving model to ./models/keras_cifar100_model.h5\n",
      "Epoch 6/165000\n",
      " 89/500 [====>.........................] - ETA: 1:38 - loss: 7.3028 - acc: 0.2217 - top_k_categorical_accuracy: 0.5192"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-949c0e70b636>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlr_rate_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_flow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     validation_steps=len(x_train) / BATCH_SIZE)\n\u001b[0m",
      "\u001b[0;32m~/Programs/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1313\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_flow_w_crops,\n",
    "                    epochs=EPOCHS,\n",
    "                    steps_per_epoch=len(x_train) / BATCH_SIZE,\n",
    "                    callbacks=[lr_rate_scheduler, dropout_scheduler, tensorboard, checkpointer],\n",
    "                    validation_data=valid_flow,\n",
    "                    validation_steps=len(x_train) / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "165.000 epochs! COME ON!\n",
    "As I mentioned earlier, I cannot finish the training process with my resources (by the way, it is 1080Ti). So, we do not have model to test at the end of this episode. If you have better GPU/s and have never ending patient during the training (for me it was expected to run at least 40 days -nonstop-), you can go for it -but I won't-."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(samplewise_center=True,\n",
    "                                  zca_whitening=True)\n",
    "test_datagen.fit(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_flow = test_datagen.flow(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "results = model.evaluate_generator(test_flow, steps=len(x_test) / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test loss: ' + str(results[0]))\n",
    "print('Accuracy: ' + str(results[1]))\n",
    "print('Top-5 Accuracy: ' + str(results[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
